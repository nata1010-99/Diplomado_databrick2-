{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c0adead-bc50-428f-bbfc-55c3928e3739",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cuaderno de ingesta de datos\n",
    "\n",
    "En este bloque traeremos desde datos abiertos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5b3eb35-af61-493f-9da8-020945ceb1e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 1: Descargar los datos y leerlos en pandas, luego convertir a Spark\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Iniciar sesi√≥n Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# URLs de los datasets\n",
    "url_secop = \"https://www.datos.gov.co/resource/rpmr-utcd.csv?$limit=100000\"\n",
    "url_men = \"https://www.datos.gov.co/resource/nudc-7mev.csv?$limit=100000\"\n",
    "\n",
    "# Funci√≥n para descargar y leer CSV desde la web\n",
    "def descargar_csv(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)  # Evita bloqueo por espera larga\n",
    "        response.raise_for_status()  # Lanza error si el c√≥digo de estado no es 200\n",
    "        return pd.read_csv(StringIO(response.text))\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al descargar los datos desde {url}:\\n{e}\")\n",
    "        return pd.DataFrame()  # Retorna un DataFrame vac√≠o si falla\n",
    "\n",
    "# Descargar y leer en pandas\n",
    "df_secop_pd = descargar_csv(url_secop)\n",
    "df_men_pd = descargar_csv(url_men)\n",
    "\n",
    "# Verificar si los DataFrames no est√°n vac√≠os antes de convertir\n",
    "if not df_secop_pd.empty and not df_men_pd.empty:\n",
    "    # Convertir a DataFrame Spark\n",
    "    df_secop = spark.createDataFrame(df_secop_pd)\n",
    "    df_men = spark.createDataFrame(df_men_pd)\n",
    "\n",
    "    # Mostrar en Databricks\n",
    "    display(df_secop)\n",
    "    display(df_men)\n",
    "else:\n",
    "    print(\"Alguno de los DataFrames est√° vac√≠o. Verifica la conexi√≥n o URLs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "845b07f3-028d-408a-9bf9-c9d910dd28c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Celda 1: Leer datos desde los archivos CSV que subiste a Volumes\n",
    "\n",
    "# Rutas locales dentro del entorno Databricks (Volumes)\n",
    "url_secop = \"/Volumes/main/diplomado_datos/manual/df_secop.csv\"\n",
    "url_men = \"/Volumes/main/diplomado_datos/manual/df_men.csv\"\n",
    "\n",
    "# Leer los archivos usando Spark\n",
    "# \"header\" indica que los nombres de las columnas est√°n en la primera fila\n",
    "# \"inferSchema\" permite que Spark adivine autom√°ticamente los tipos de datos\n",
    "df_secop = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(url_secop)\n",
    "df_men = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(url_men)\n",
    "\n",
    "# Mostrar los primeros registros en Databricks usando .show()\n",
    "print(\"Datos del SECOP cargados:\")\n",
    "df_secop.show()\n",
    "\n",
    "print(\"Datos del MEN cargados:\")\n",
    "df_men.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a795b3-dcf2-4ab7-b3c9-d59442e0f11b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_men.limit(10))\n",
    "display(df_secop.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ac6b21-83e5-447d-a231-4bb6e693c2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_secop.count()\n",
    "df_men.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "565f05f7-7495-4ea6-a1a7-3c99a6d92167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Celda 2: Guardar los DataFrames como tablas Delta\n",
    "\n",
    "# La funci√≥n .saveAsTable() guarda los datos y registra la tabla en el Unity Catalog.\n",
    "# El modo \"overwrite\" reemplaza la tabla si ya existe, ideal para actualizaciones.\n",
    "\n",
    "df_secop.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main.diplomado_datos.secop\")\n",
    "df_men.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main.diplomado_datos.men_estadisticas\")\n",
    "\n",
    "print(\"¬°Tablas guardadas exitosamente en el cat√°logo 'main', esquema 'diplomado_datos'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a63c6e-2e33-418e-9f44-c03f845c045a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar librer√≠as necesarias\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta local del archivo\n",
    "file_path = \"/Volumes/main/diplomado_datos/manual/df_secop.csv\"\n",
    "\n",
    "# Leer el archivo CSV con pandas usando coma como delimitador\n",
    "df_secop_pd = pd.read_csv(file_path, delimiter=',', header=0, on_bad_lines='skip')\n",
    "print(df_secop_pd.columns.tolist())\n",
    "\n",
    "# üîß Limpieza de texto y filas incompletas\n",
    "for col in [\"objeto_a_contratar\", \"objeto_del_proceso\"]:\n",
    "    df_secop_pd[col] = df_secop_pd[col].astype(str).str.replace(\"\\n\", \" \").str.replace('\"', \"\").str.strip()\n",
    "\n",
    "df_secop_pd = df_secop_pd.dropna(thresh=10)  # Elimina filas con menos de 10 columnas v√°lidas\n",
    "\n",
    "# Convertir pandas a Spark\n",
    "df_secop = spark.createDataFrame(df_secop_pd)\n",
    "\n",
    "# Mostrar los datos limpios\n",
    "display(df_secop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf80a6c8-ddb9-4807-bdd9-297480ac40b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fe5b8cd-5aa7-454d-b9ce-5b463e58966b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# üß† Obtener el esquema de la tabla destino\n",
    "target_schema = spark.table(\"main.diplomado_datos.secop\").schema\n",
    "\n",
    "# üîÑ Alinear cada columna del DataFrame con el tipo de dato de la tabla\n",
    "df_secop_aligned = df_secop.select(\n",
    "    [col(field.name).cast(field.dataType) for field in target_schema.fields]\n",
    ")\n",
    "\n",
    "# üìù Escribir en la tabla Delta usando append y mergeSchema\n",
    "df_secop_aligned.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(\"main.diplomado_datos.secop\")\n",
    "\n",
    "print(\"‚úÖ Datos a√±adidos correctamente a la tabla 'main.diplomado_datos.secop'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "229b8c12-5dce-4102-84e0-189559c20e56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# üìÇ Ruta basada en tu ubicaci√≥n actual en Databricks\n",
    "ruta_base = \"/Volumes/main/diplomado_datos/manual\"\n",
    "\n",
    "# üìå Buscar todos los archivos que comienzan con 'secop_lote_' y terminan en '.csv'\n",
    "archivos_lotes = [f for f in os.listdir(ruta_base) if f.startswith(\"secop_lote_\") and f.endswith(\".csv\")]\n",
    "\n",
    "# üî¢ Ordenar los archivos por n√∫mero de lote (usando el n√∫mero en el nombre)\n",
    "archivos_lotes.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "# üß© Combinar todos los archivos en un solo DataFrame\n",
    "df_secop = pd.concat([\n",
    "    pd.read_csv(os.path.join(ruta_base, archivo), low_memory=False)\n",
    "    for archivo in archivos_lotes\n",
    "], ignore_index=True)\n",
    "\n",
    "# üìä Verificaci√≥n r√°pida\n",
    "print(f\"‚úÖ Total de registros combinados: {df_secop.shape[0]}\")\n",
    "print(f\"üî¨ Columnas disponibles: {df_secop.columns.tolist()}\")\n",
    "\n",
    "# üßº (Opcional) Validar duplicados y nulos\n",
    "print(f\"üîç Duplicados: {df_secop.duplicated().sum()}\")\n",
    "print(\"üßπ Nulos por columna:\")\n",
    "print(df_secop.isnull().sum())\n",
    "\n",
    "# üíæ Guardar el archivo combinado en la misma ruta\n",
    "df_secop.to_csv(os.path.join(ruta_base, \"secop_completo.csv\"), index=False)\n",
    "print(\"üì¶ Archivo combinado guardado como 'secop_completo.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a622d5c-f7f3-4828-b917-2da89bb2af61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = spark.read.csv(\n",
    "    path=\"/Volumes/main/diplomado_datos/manual/secop_lote_*.csv\",\n",
    "    header=True,\n",
    "    sep=\",\",\n",
    "    quote='\"',\n",
    "    escape='\"',\n",
    "    multiLine=True,\n",
    "    encoding=\"UTF-8\",\n",
    "    inferSchema=False\n",
    ")\n",
    "\n",
    "# Verificar n√∫mero de columnas\n",
    "print(len(df_raw.columns))\n",
    "print(df_raw.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c7e8f5-89ab-4807-9152-50d92a6cadbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_limpio = df_raw.dropna(subset=[\n",
    "    \"valor_contrato\",\n",
    "    \"fecha_de_firma_del_contrato\",\n",
    "    \"fecha_inicio_ejecuci_n\",\n",
    "    \"fecha_fin_ejecuci_n\",\n",
    "    \"documento_proveedor\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94fe2cba-bea4-41d2-90ee-f43512ace5bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, regexp_replace\n",
    "\n",
    "df_limpio = df_limpio.withColumn(\"valor_contrato\", regexp_replace(\"valor_contrato\", \"[^0-9]\", \"\").cast(\"long\"))\n",
    "\n",
    "df_limpio = df_limpio.withColumn(\"fecha_de_firma_del_contrato\", to_timestamp(\"fecha_de_firma_del_contrato\"))\n",
    "df_limpio = df_limpio.withColumn(\"fecha_inicio_ejecuci_n\", to_timestamp(\"fecha_inicio_ejecuci_n\"))\n",
    "df_limpio = df_limpio.withColumn(\"fecha_fin_ejecuci_n\", to_timestamp(\"fecha_fin_ejecuci_n\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89d66e19-ca43-49f6-8f8f-c851a2221647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Filtrar solo las filas con fechas v√°lidas en el formato YYYY-MM-DDTHH:MM:SS\n",
    "df_limpio = df_raw.filter(\n",
    "    col(\"fecha_de_firma_del_contrato\").rlike(r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\")\n",
    ")\n",
    "\n",
    "# Convertir esa columna a timestamp\n",
    "df_limpio = df_limpio.withColumn(\n",
    "    \"fecha_de_firma_del_contrato\", to_timestamp(\"fecha_de_firma_del_contrato\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92aea3b0-57ab-4fc8-a640-b8c4996d34a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Primero, filtrar solo las filas con fechas v√°lidas en las 3 columnas\n",
    "df_fechas_validas = df_raw.filter(\n",
    "    col(\"fecha_de_firma_del_contrato\").rlike(r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\") &\n",
    "    col(\"fecha_inicio_ejecuci_n\").rlike(r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\") &\n",
    "    col(\"fecha_fin_ejecuci_n\").rlike(r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\")\n",
    ")\n",
    "\n",
    "# Luego convertirlas a timestamp\n",
    "df_limpio = (\n",
    "    df_fechas_validas\n",
    "    .withColumn(\"fecha_de_firma_del_contrato\", to_timestamp(\"fecha_de_firma_del_contrato\"))\n",
    "    .withColumn(\"fecha_inicio_ejecuci_n\", to_timestamp(\"fecha_inicio_ejecuci_n\"))\n",
    "    .withColumn(\"fecha_fin_ejecuci_n\", to_timestamp(\"fecha_fin_ejecuci_n\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c21c1673-c93c-44b9-8ef5-0f7ec9e78a04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "df_limpio = df_limpio.withColumn(\n",
    "    \"fecha_de_firma_del_contrato\",\n",
    "    to_timestamp(\"fecha_de_firma_del_contrato\")\n",
    ")\n",
    "df_limpio = df_limpio.withColumn(\n",
    "    \"fecha_inicio_ejecuci_n\",\n",
    "    to_timestamp(\"fecha_inicio_ejecuci_n\")\n",
    ").withColumn(\n",
    "    \"fecha_fin_ejecuci_n\",\n",
    "    to_timestamp(\"fecha_fin_ejecuci_n\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf27b26-48f6-43a0-9f86-0792ad55373b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_limpio.select(\n",
    "    \"fecha_de_firma_del_contrato\", \n",
    "    \"fecha_inicio_ejecuci_n\", \n",
    "    \"fecha_fin_ejecuci_n\"\n",
    ").show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a2ea8e-ed8b-440d-81f9-4e69ad3cc947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when, isnan, trim\n",
    "\n",
    "columnas_clave = [\n",
    "    \"valor_contrato\",\n",
    "    \"nombre_de_la_entidad\",\n",
    "    \"numero_de_proceso\",\n",
    "    \"tipo_documento_proveedor\",\n",
    "    \"documento_proveedor\"\n",
    "]\n",
    "\n",
    "df_limpio.select([\n",
    "    count(\n",
    "        when(\n",
    "            col(c).isNull() | (trim(col(c)) == \"\"),\n",
    "            c\n",
    "        )\n",
    "    ).alias(f\"{c}_vacios\")\n",
    "    for c in columnas_clave\n",
    "]).show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingesta_Datos_Abiertos",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

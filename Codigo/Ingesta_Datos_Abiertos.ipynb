{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c0adead-bc50-428f-bbfc-55c3928e3739",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cuaderno de ingesta de datos\n",
    "\n",
    "En este bloque traeremos desde datos abiertos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5b3eb35-af61-493f-9da8-020945ceb1e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 1: Descargar los datos y leerlos en pandas, luego convertir a Spark\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Iniciar sesión Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# URLs de los datasets\n",
    "url_secop = \"https://www.datos.gov.co/resource/rpmr-utcd.csv?$limit=100000\"\n",
    "url_men = \"https://www.datos.gov.co/resource/nudc-7mev.csv?$limit=100000\"\n",
    "\n",
    "# Función para descargar y leer CSV desde la web\n",
    "def descargar_csv(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)  # Evita bloqueo por espera larga\n",
    "        response.raise_for_status()  # Lanza error si el código de estado no es 200\n",
    "        return pd.read_csv(StringIO(response.text))\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al descargar los datos desde {url}:\\n{e}\")\n",
    "        return pd.DataFrame()  # Retorna un DataFrame vacío si falla\n",
    "\n",
    "# Descargar y leer en pandas\n",
    "df_secop_pd = descargar_csv(url_secop)\n",
    "df_men_pd = descargar_csv(url_men)\n",
    "\n",
    "# Verificar si los DataFrames no están vacíos antes de convertir\n",
    "if not df_secop_pd.empty and not df_men_pd.empty:\n",
    "    # Convertir a DataFrame Spark\n",
    "    df_secop = spark.createDataFrame(df_secop_pd)\n",
    "    df_men = spark.createDataFrame(df_men_pd)\n",
    "\n",
    "    # Mostrar en Databricks\n",
    "    display(df_secop)\n",
    "    display(df_men)\n",
    "else:\n",
    "    print(\"Alguno de los DataFrames está vacío. Verifica la conexión o URLs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "845b07f3-028d-408a-9bf9-c9d910dd28c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Celda 1: Leer datos desde los archivos CSV que subiste a Volumes\n",
    "\n",
    "# Rutas locales dentro del entorno Databricks (Volumes)\n",
    "url_secop = \"/Volumes/main/diplomado_datos/manual/df_secop.csv\"\n",
    "url_men = \"/Volumes/main/diplomado_datos/manual/df_men.csv\"\n",
    "\n",
    "# Leer los archivos usando Spark\n",
    "# \"header\" indica que los nombres de las columnas están en la primera fila\n",
    "# \"inferSchema\" permite que Spark adivine automáticamente los tipos de datos\n",
    "df_secop = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(url_secop)\n",
    "df_men = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(url_men)\n",
    "\n",
    "# Mostrar los primeros registros en Databricks usando .show()\n",
    "print(\"Datos del SECOP cargados:\")\n",
    "df_secop.show()\n",
    "\n",
    "print(\"Datos del MEN cargados:\")\n",
    "df_men.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a795b3-dcf2-4ab7-b3c9-d59442e0f11b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_men.limit(10))\n",
    "display(df_secop.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ac6b21-83e5-447d-a231-4bb6e693c2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_secop.count()\n",
    "df_men.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "565f05f7-7495-4ea6-a1a7-3c99a6d92167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Celda 2: Guardar los DataFrames como tablas Delta\n",
    "\n",
    "# La función .saveAsTable() guarda los datos y registra la tabla en el Unity Catalog.\n",
    "# El modo \"overwrite\" reemplaza la tabla si ya existe, ideal para actualizaciones.\n",
    "\n",
    "df_secop.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main.diplomado_datos.secop\")\n",
    "df_men.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main.diplomado_datos.men_estadisticas\")\n",
    "\n",
    "print(\"¡Tablas guardadas exitosamente en el catálogo 'main', esquema 'diplomado_datos'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4336c5ff-cea2-451c-89d2-1fa97afdbd5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear sesión de Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# URL del dataset\n",
    "url_secop = \"https://www.datos.gov.co/resource/rpmr-utcd.csv?$limit=100000&$offset=100000\"\n",
    "\n",
    "# Descargar contenido\n",
    "response_secop = requests.get(url_secop)\n",
    "\n",
    "# Leer CSV en pandas\n",
    "df_secop_pd = pd.read_csv(StringIO(response_secop.text), delimiter=\",\", low_memory=False)\n",
    "\n",
    "# Limpiar nombres de columnas (opcional pero recomendado)\n",
    "df_secop_pd.columns = [col.strip().lower().replace(\" \", \"_\") for col in df_secop_pd.columns]\n",
    "\n",
    "# Verifica que los datos se cargaron correctamente en pandas\n",
    "print(df_secop_pd.head())\n",
    "\n",
    "# Convertir a Spark DataFrame\n",
    "df_secop = spark.createDataFrame(df_secop_pd)\n",
    "\n",
    "# Mostrar en Databricks\n",
    "display(df_secop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf80a6c8-ddb9-4807-bdd9-297480ac40b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fe5b8cd-5aa7-454d-b9ce-5b463e58966b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Get the target schema\n",
    "target_schema = spark.table(\"main.diplomado_datos.secop\").schema\n",
    "\n",
    "# Select and cast columns that exist in both df_secop and target_schema\n",
    "df_secop_aligned = df_secop.select(\n",
    "    [col(field.name).cast(field.dataType) for field in target_schema.fields if field.name in df_secop.columns]\n",
    ")\n",
    "\n",
    "# Write the aligned DataFrame to the Delta table\n",
    "df_secop_aligned.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(\"main.diplomado_datos.secop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "845db4d8-3824-4f87-b2db-992f9d321798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_registros = 19446266\n",
    "offset_inicial = 200000\n",
    "limite = 100000\n",
    "paginas_faltantes = ((total_registros - offset_inicial) // limite) + 1\n",
    "\n",
    "print(f\"Quedan {paginas_faltantes} bloques por descargar...\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingesta_Datos_Abiertos",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
